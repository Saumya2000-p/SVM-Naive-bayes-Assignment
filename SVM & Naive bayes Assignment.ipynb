{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "64bdb8d3-d1ea-4b48-8ccf-082664a6937f",
      "cell_type": "code",
      "source": "                                                    ###SVM & Naive bayes Assignment###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a524837e-91bf-4688-bed4-e196aba78ac2",
      "cell_type": "code",
      "source": "###Theoretical\n\n1. What is a Support Vector Machine (SVM)?\n2. What is the difference between Hard Margin and Soft Margin SVM?\n3. What is the mathematical intuition behind SVM?\n4. What is the role of Lagrange Multipliers in SVM?\n5. What are Support Vectors in SVM?\n6. What is a Support Vector Classifier (SVC)?\n7. What is a Support Vector Regressor (SVR)?\n8. What is the Kernel Trick in SVM?\n9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n10. What is the effect of the C parameter in SVM?\n11. What is the role of the Gamma parameter in RBF Kernel SVM?\n12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n13. What is Bayes' Theorem?\n14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naive Bayes, and Bernoulli Naïve Bayes.\n15. When should you use Gaussian Naïve Bayes over other variants?\n16. What are the key assumptions made by Naïve Bayes?\n17. What are the advantages and disadvantages of Naive Bayes?\n18. Why is Naïve Bayes a good choice for text classification?\n19. Compare SVM and Naïve Bayes for classification tasks.\n20. How does Laplace Smoothing help in Naive Bayes?\n\n###Practical\n\n21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE).\n24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n26. Write a Python program to train a Multinomial Naive Bayes classifier for text classification using the 20 Newsgroups dataset.\n27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n28. Write a Python program to train a Bernoulli Naive Bayes classifier for binary classification on a dataset with binary features.\n29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data,\n30. Write a Python program to train a Gaussian Naive Bayes model and compare the predictions before and after Laplace Smoothing.\n31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy.\n33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data.\n34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n35. Write a Python program to perform feature selection before training a Naive Bayes classifier and compare results.\n36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance.\n40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and Fl-Score instead of accuracy.\n42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn..\n44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bf7ee900-e44c-48ea-a9ce-d8db335cbd05",
      "cell_type": "code",
      "source": "Answer1:-A Support Vector Machine (SVM) is a supervised learning algorithm that can be used for classification or regression tasks. It aims to find the hyperplane that maximally separates the classes in the feature space.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a32b2901-e4a6-4f51-8e7f-06921d8f401a",
      "cell_type": "code",
      "source": "Answer2:-Hard Margin SVM requires that all data points be classified correctly, whereas Soft Margin SVM allows for some misclassifications by introducing slack variables.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c5463f85-8d7f-4aa7-ad93-961e56c8791f",
      "cell_type": "code",
      "source": "Answer3:-SVM aims to find the hyperplane that maximizes the margin between classes. The margin is defined as the distance between the hyperplane and the closest data points (support vectors).\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e5de5704-aa76-44b5-aa8c-0806143fca20",
      "cell_type": "code",
      "source": "Answer4:-Lagrange Multipliers are used to convert the constrained optimization problem of SVM into an unconstrained problem, which can be solved more efficiently.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c340a1ff-8a5e-47fd-b3e1-5710f2c0de3d",
      "cell_type": "code",
      "source": "Answer5:-Support Vectors are the data points that lie closest to the hyperplane and define the margin. They are the most important points in determining the decision boundary.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35cce890-2b34-4627-a367-a1e8e2948f6a",
      "cell_type": "code",
      "source": "Answer6:-A Support Vector Classifier (SVC) is a type of SVM used for classification tasks.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "28da27c3-17f9-43e6-810f-341ef820aa00",
      "cell_type": "code",
      "source": "Answer7:-A Support Vector Regressor (SVR) is a type of SVM used for regression tasks.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "582568b7-8a07-4cdd-966c-639753c24ad7",
      "cell_type": "code",
      "source": "Answer8:-The Kernel Trick is a technique used to transform the original feature space into a higher-dimensional space, where the data becomes linearly separable.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7fb6c0d7-38a8-4456-a7e2-d3ab8f5d8f0b",
      "cell_type": "code",
      "source": "Answer9:-Linear Kernel is used for linearly separable data, Polynomial Kernel is used for non-linearly separable data, and RBF (Radial Basis Function) Kernel is used for data that requires a Gaussian-like decision boundary.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6093a3c3-bb73-448f-9919-7ddf5bf7c57b",
      "cell_type": "code",
      "source": "Answer10:-The C parameter controls the trade-off between margin and misclassification error. A small C value leads to a smooth decision boundary, while a large C value leads to a more complex decision boundary.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "67ca8d7c-a7f6-43ed-b903-59156d078452",
      "cell_type": "code",
      "source": "Answer11:-The Gamma parameter determines the influence of the support vectors on the decision boundary. A small Gamma value leads to a broader decision boundary, while a large Gamma value leads to a more complex decision boundary.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f12dde20-dfe8-46d3-8e8e-1abbe5119568",
      "cell_type": "code",
      "source": "Answer12:-The Naïve Bayes classifier is a probabilistic classifier based on Bayes' Theorem. It is called \"Naïve\" because it assumes independence between features.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7ed8223d-9110-451c-b79c-7056e03ac8aa",
      "cell_type": "code",
      "source": "Answer13:-Bayes' Theorem is a mathematical formula for updating the probability of a hypothesis based on new evidence.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c09becc0-f2f8-480b-aa8e-80b871bc0bf5",
      "cell_type": "code",
      "source": "Answer14:-Gaussian Naïve Bayes assumes a Gaussian distribution for features, Multinomial Naive Bayes assumes a multinomial distribution for features, and Bernoulli Naïve Bayes assumes a Bernoulli distribution for binary features.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4cec039d-8272-4ee7-95fc-36f30250d4d2",
      "cell_type": "code",
      "source": "Answer15:-Gaussian Naïve Bayes is suitable for continuous features that follow a Gaussian distribution.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9490fc13-b241-420c-a569-6f21ecab6f5a",
      "cell_type": "code",
      "source": "Answer16:-The key assumptions made by Naïve Bayes are independence between features and a specific distribution for features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ffc997b0-fc03-4ace-9fcd-8a0256ef14c3",
      "cell_type": "code",
      "source": "Answer17:-Advantages include simplicity and efficiency, while disadvantages include the assumption of independence between features.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5d61630-3248-48b8-afa1-59664ebd9609",
      "cell_type": "code",
      "source": "Answer18:-Naïve Bayes is a good choice for text classification because it can handle high-dimensional data and is efficient to train.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ecfa4592-e056-4865-b6e6-836a66826e39",
      "cell_type": "code",
      "source": "Answer19:-SVM is more robust to noise and can handle non-linearly separable data, while Naïve Bayes is simpler and more efficient.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ad22adbe-f802-434d-935f-a3d70ab48510",
      "cell_type": "code",
      "source": "Answer20:-Laplace Smoothing helps to avoid zero probabilities for features that are not present in the training data.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d24204a7-fa80-4f62-8649-af8aaeae5e72",
      "cell_type": "code",
      "source": "Answer21:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "16a1bf33-be63-4c6c-82b3-d63bd400885e",
      "cell_type": "code",
      "source": "Answer22:-from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX = wine.data\ny = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = SVC(kernel='linear')\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = SVC(kernel='rbf')\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (Linear Kernel): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (RBF Kernel): {accuracy_score(y_test, y_pred2):.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "345b5b1d-efe9-4bcd-992d-9102fbac6ad9",
      "cell_type": "code",
      "source": "Answer23:-from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\ncal_housing = fetch_california_housing()\nX = cal_housing.data\ny = cal_housing.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = SVR()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20688992-7f2f-4683-8bba-d518989cbb7e",
      "cell_type": "code",
      "source": "Answer24:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import SVC\n\nX, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std=1.05, random_state=40)\nmodel = SVC(kernel='poly', degree=3)\nmodel.fit(X, y)\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e3327aa2-1e62-4f5b-bb26-5276d4f35194",
      "cell_type": "code",
      "source": "Answer25:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7e811f81-10d1-4d15-9a49-966936cecc37",
      "cell_type": "code",
      "source": "Answer26:-from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\n\nnewsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\nX = newsgroups.data\ny = newsgroups.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nvectorizer = CountVectorizer()\nX_train_count = vectorizer.fit_transform(X_train)\nX_test_count = vectorizer.transform(X_test)\n\nmodel = MultinomialNB()\nmodel.fit(X_train_count, y_train)\ny_pred = model.predict(X_test_count)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5eb67936-e433-433d-b8fc-8c86f6f9b9a1",
      "cell_type": "code",
      "source": "Answer27:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import SVC\n\nX, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std=1.05, random_state=40)\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, C in enumerate([0.1, 1, 10]):\n    model = SVC(C=C)\n    model.fit(X, y)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    axs[i].contourf(xx, yy, Z, alpha=0.8)\n    axs[i].scatter(X[:, 0], X[:, 1], c=y)\n    axs[i].set_title(f'C = {C}')\n\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "45287e32-97d5-4866-a79a-5017bca62051",
      "cell_type": "code",
      "source": "Answer28:-from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\n\nX, y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=0, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = BernoulliNB()\nmodel.fit(X_train > 0, y_train)\ny_pred = model.predict(X_test > 0)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32473519-3c1f-41a6-b806-2c2d9421dc1a",
      "cell_type": "code",
      "source": "Answer29:-from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX = wine.data\ny = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel1 = SVC()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = SVC()\nmodel2.fit(X_train_scaled, y_train)\ny_pred2 = model2.predict(X_test_scaled)\n\nprint(f'Accuracy (unscaled): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (scaled): {accuracy_score(y_test, y_pred2):.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "942e4dae-1c9c-4f76-9566-35834b315349",
      "cell_type": "code",
      "source": "Answer30:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = GaussianNB(var_smoothing=0)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = GaussianNB(var_smoothing=1e-9)\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (no smoothing): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (with smoothing): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "58183ff7-b41f-4d2e-93ac-ecbdd09a6023",
      "cell_type": "code",
      "source": "Answer31:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'gamma': ['scale', 'auto'],\n    'kernel': ['linear', 'rbf', 'poly']\n}\n\nmodel = SVC()\ngrid_search = GridSearchCV(model, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(f'Best Parameters: {grid_search.best_params_}')\nprint(f'Best Score: {grid_search.best_score_:.2f}')\n\nbest_model = grid_search.best_estimator_\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "29ecac0d-37d7-44bc-a5a7-76d4ef55428a",
      "cell_type": "code",
      "source": "Answer32:-from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, weights=[0.9, 0.1], random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = SVC()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = SVC(class_weight='balanced')\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (unweighted): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Classification Report (unweighted):\\n{classification_report(y_test, y_pred1)}')\n\nprint(f'Accuracy (weighted): {accuracy_score(y_test, y_pred2):.2f}')\nprint(f'Classification Report (weighted):\\n{classification_report(y_test, y_pred2)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "15748e92-76d5-4f47-90ce-875e5a14e159",
      "cell_type": "code",
      "source": "Answer33:-import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Sample dataset\nemails = ['This is a spam email', 'This is a ham email', 'Buy now and get discount', 'Meeting at 2 PM']\nlabels = [1, 0, 1, 0]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(emails)\ny = np.array(labels)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ae8b8e4f-6937-4f2f-8eec-82c6f481405f",
      "cell_type": "code",
      "source": "Answer34:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = SVC()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = GaussianNB()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'SVM Accuracy: {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Naive Bayes Accuracy: {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "241ede0e-2b33-49b4-849b-f753f2ea4557",
      "cell_type": "code",
      "source": "Answer35:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nselector = SelectKBest(f_classif, k=2)\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n\nmodel = GaussianNB()\nmodel.fit(X_train_selected, y_train)\ny_pred = model.predict(X_test_selected)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4fc7c67-98c7-4d92-b8c5-3eaab8e4e269",
      "cell_type": "code",
      "source": "Answer36:-from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX = wine.data\ny = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = OneVsRestClassifier(SVC())\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = OneVsOneClassifier(SVC())\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (OvR): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (OvO): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dafb4dae-8f30-4f31-a3bc-2bb3d382255f",
      "cell_type": "code",
      "source": "Answer37:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = SVC(kernel='linear')\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = SVC(kernel='poly')\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nmodel3 = SVC(kernel='rbf')\nmodel3.fit(X_train, y_train)\ny_pred3 = model3.predict(X_test)\n\nprint(f'Accuracy (Linear): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (Polynomial): {accuracy_score(y_test, y_pred2):.2f}')\nprint(f'Accuracy (RBF): {accuracy_score(y_test, y_pred3):.2f}')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f3468158-b52b-4f35-b465-803d09a44748",
      "cell_type": "code",
      "source": "Answer38:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccuracies = []\n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = SVC()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracies.append(accuracy_score(y_test, y_pred))\n\nprint(f'Average Accuracy: {np.mean(accuracies):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "79148d3f-f66d-48b7-b215-6121934346c6",
      "cell_type": "code",
      "source": "Answer39:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Default prior probabilities\nmodel1 = MultinomialNB()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\n# Custom prior probabilities\npriors = [0.4, 0.3, 0.3]\nmodel2 = MultinomialNB(class_prior=priors)\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (default priors): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (custom priors): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f62f6ff-0336-470f-997e-8544ef18de6c",
      "cell_type": "code",
      "source": "Answer40:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nestimator = SVC()\nselector = RFE(estimator, n_features_to_select=10)\nselector.fit(X_train, y_train)\n\nX_train_selected = selector.transform(X_train)\nX_test_selected = selector.transform(X_test)\n\nmodel = SVC()\nmodel.fit(X_train_selected, y_train)\ny_pred = model.predict(X_test_selected)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31122f6f-76ea-4ea1-892c-764d07af1371",
      "cell_type": "code",
      "source": "Answer41:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(f'Precision: {precision_score(y_test, y_pred, average=\"weighted\"):.2f}')\nprint(f'Recall: {recall_score(y_test, y_pred, average=\"weighted\"):.2f}')\nprint(f'F1-Score: {f1_score(y_test, y_pred, average=\"weighted\"):.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9403241d-9ec4-4ec3-ae1a-ea5a8e083f2d",
      "cell_type": "code",
      "source": "Answer42:-from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import log_loss\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred_proba = model.predict_proba(X_test)\n\nprint(f'Log Loss: {log_loss(y_test, y_pred_proba):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b42b6251-17c8-4644-b20e-d7ad4d404e59",
      "cell_type": "code",
      "source": "Answer43:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = SVC(probability=True)\nmodel.fit(X_train, y_train)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b6781a3d-15a8-44cf-adb1-6fda6384f96b",
      "cell_type": "code",
      "source": "Answer44:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f649fe18-6d9f-4b31-b354-cf500b4262ba",
      "cell_type": "code",
      "source": "Answer45:-from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX = wine.data\ny = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = SVC()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = GaussianNB()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'SVM Accuracy: {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Naive Bayes Accuracy: {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0e6bf904-48a1-4038-86e3-a71887b217cc",
      "cell_type": "code",
      "source": "Answer46:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nkernels = ['linear', 'poly', 'rbf']\nfor kernel in kernels:\n    model = SVC(kernel=kernel)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f'Accuracy ({kernel}): {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}